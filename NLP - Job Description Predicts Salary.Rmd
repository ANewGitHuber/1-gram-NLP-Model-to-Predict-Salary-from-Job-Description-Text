---
title: |
  | A Unigram NLP Model to Predict Salary from Job Description Text
author: |
  | The Business School, Imperial College London
  | John Chen

date: "03-02-2024"
output: pdf_document
---
\newpage
\tableofcontents
\listoffigures
\listoftables
\newpage

# Question 1

## Preparation
Load packages
```{r, message=F, warning=F}
library(quanteda)
library(ggrepel)
library(textclean)
library(tidyverse)
library(glmnet)
library(pROC)
library(sentimentr)
library(dplyr)
```

Source functions
```{r, message=F, warning=F}
source("TMEF_dfm.R")
source("kendall_acc.R")
```

## 1-gram model to predict job salary from job description
```{r}
# Read Data
job <- readRDS("jd_small.RDS")

# Training & Testing data Split
# Train - 8000 rows, Test - 2000 rows
set.seed(0)
train_split = sample(1:nrow(job),8000)
job_train <- job[train_split,]
job_test <- job[-train_split,]

# create prediction variables from the job description text, N-1 (Unigram)
job_train_dfm <- TMEF_dfm(job_train$FullDescription , ngrams=1) %>%
  convert(to="matrix")

# Extract the salary column (to be predicted)
job_train_Y <- job_train %>%
  pull(SalaryNormalized)

# Put training data into LASSO model
job_model<-cv.glmnet(x=job_train_dfm,
                     y=job_train_Y)

# check the Lasso tuning
plot(job_model)
```
It should be noticed that the astronomical numbers in MSE (y-axis) seem weird but make sense because we are predicting the salary, which is a large number. For example, if I predict a salary being 30000 for a true value of 100000, the error is 70000 and the corresponding MSE is 70000^2. The result would be 4.9*10^9, similar to the value in our MSE plot.

## Interpret with a coefficient plot
```{r message=FALSE, warning=FALSE, results='hide'}
plotDat<-job_model %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  %>%
  # add ngram frequencies for plotting
  left_join(data.frame(ngram=colnames(job_train_dfm),
                       freq=colMeans(job_train_dfm)))

plotDat %>%
  mutate_at(vars(score,freq),~round(.,3))

plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient2(low="navyblue",
                       mid = "grey",
                       high="forestgreen",
                       midpoint = 0)+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps=40, force=6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(.01,.05,.1,.2,.5,1,2,5))+
  theme_bw() +
  labs(x="Coefficient in Model",y="Uses per Review")+
  theme(legend.position = "none",
        axis.title=element_text(size=20),
        axis.text=element_text(size=16))
```
## What did the model find
Feature predicting high salaries: leadership, vision, director...
Feature predicting low salaries: graduate, assistant...

# Question 2

## Evaluate model accuracy
```{r}
# Test the model on testing dataset
job_test_dfm <- TMEF_dfm(job_test$FullDescription,
                         ngrams=1,
                         min.prop=0) %>%
  dfm_match(colnames(job_train_dfm)) %>%
  convert(to="matrix")

# Make predictions
job_predict<-predict(job_model, newx = job_test_dfm)[,1]

# Evaluate accuracy based
acc_model <- kendall_acc(job_test$SalaryNormalized,job_predict)

# Output
acc_model
```
The accuracy of the model is 74.84%.

## Create two benchmarks
```{r}
# First Benchmark - The number of words in job description
job_test <- job_test %>%
  mutate(word_count = str_count(FullDescription,"[[:alpha:]]+"))

# Second Benchmark - re-random the prediction
job_test <- job_test %>%
  mutate(model_random = sample(job_predict))

# Accuracy of the first benchmark
acc_word_count <- kendall_acc(job_test$SalaryNormalized,
                              job_test$word_count)

# Output
acc_word_count

# Accuracy of the second benchmark
acc_random <- kendall_acc(job_test$SalaryNormalized,
                              job_test$model_random)

# Output
acc_random
```
## Plot the model accuracy with the two benchmarks in a graph
```{r}
## Combine the model accuracy and the two benchmarks' accuracy
acc_report <- bind_rows(acc_model %>%
                          mutate(field="Original Model"),
                        acc_word_count %>%
                          mutate(field="Benchmark 1 - Word Count"),
                        acc_random %>%
                          mutate(field="Benchmark 2 - Re-Random"))

acc_report %>% 
  ggplot(aes(x=field,color=field,
             y=acc,ymin=lower,ymax=upper)) +
  geom_point() +
  geom_errorbar(width=.4) +
  theme_bw() +
  labs(x="Test Data",y="Accuracy (%)") +
  geom_hline(yintercept = 50) +
  theme(panel.grid=element_blank(),
        legend.position="none")
```
The graph shows that our model's accuracy is among the highest between itself and the two benchmarks.

# Question 3

## Great prediction samples for predicting high salary
```{r}
# Store predictions in data, calculate accuracy
job_test <- job_test %>%
  mutate(prediction = job_predict,
         error = abs(SalaryNormalized - prediction),
         bias = SalaryNormalized- prediction)

# Filter great predictions that predict high salary
great_prediction_high <- job_test %>%
  filter(SalaryNormalized == 80000 & error < 20000) %>%
  select(FullDescription, SalaryNormalized, prediction)

# Output great predictions that predict high salary
great_prediction_high

# Output two of the great predictions that predict high salary
# As the texts are very long, extract the first 200 words in the text
great_prediction_high %>%
  mutate(First200Words = sapply(strsplit(FullDescription, "\\s+"), function(words) {
    words_cleaned <- words[words != ""] # Remove any empty spaces
    paste(head(words_cleaned, 200), collapse = " ")
  }
  ))%>%
  slice(1:2) %>%
  pull(First200Words)
```

## Great prediction samples for predicting low salary
```{r}
# Filter great predictions that predict low salary
great_prediction_low <- job_test %>%
  filter(SalaryNormalized == 15000 & error < 10000) %>%
  select(FullDescription, SalaryNormalized, prediction)

# Output great predictions that predict high salary
great_prediction_low

# Output two of the great predictions that predict high salary
# As the texts are very long, extract the first 200 words in the text
great_prediction_low %>%
  mutate(First200Words = sapply(strsplit(FullDescription, "\\s+"), function(words) {
    words_cleaned <- words[words != ""] # Remove any empty spaces
    paste(head(words_cleaned, 200), collapse = " ")
  }
  ))%>%
  slice(1:2) %>%
  pull(First200Words)
```


# Question 4

## Bad prediction samples for predicting high salary
```{r}
# Plot prediction distribution
job_test %>%
  ggplot(aes(x=prediction)) +
  geom_histogram()

# Plot real salary distribution
job_test %>%
  ggplot(aes(x=SalaryNormalized)) +
  geom_histogram()

# Filter bad predictions that predict high salary
bad_prediction_high <- job_test %>%
  arrange(bias) %>%
  slice(1:10) %>%
  select(FullDescription, SalaryNormalized, prediction)

# Output bad predictions that predict high salary
bad_prediction_high

# Output two of the bad predictions that predict high salary
# As the texts are very long, extract the first 200 words in the text
bad_prediction_high %>%
  mutate(First200Words = sapply(strsplit(FullDescription, "\\s+"), function(words) {
    words_cleaned <- words[words != ""] # Remove any empty spaces
    paste(head(words_cleaned, 200), collapse = " ")
  }
  ))%>%
  slice(1:2) %>%
  pull(First200Words)
```

## Bad prediction samples for predicting low salary
```{r}
# Filter bad predictions that predict low salary
bad_prediction_low <- job_test %>%
  arrange(-bias) %>%
  slice(1:10) %>%
  select(FullDescription, SalaryNormalized, prediction)

# Output bad predictions that predict low salary
bad_prediction_low

# Output two of the bad predictions that predict low salary
# As the texts are very long, extract the first 200 words in the text
bad_prediction_low %>%
  mutate(First200Words = sapply(strsplit(FullDescription, "\\s+"), function(words) {
    words_cleaned <- words[words != ""] # Remove any empty spaces
    paste(head(words_cleaned, 200), collapse = " ")
  }
  ))%>%
  slice(1:2) %>%
  pull(First200Words)
```

# Question 5

## Possible improvement on models based on question 3 and 4

Positive sentimental words do have a positive impact on the prediction of high salaries, but because companies usually use a large number of positive words to beautify the description of any position, these words can significantly mislead the accuracy of high salary predictions in a specific industry.

For example, from the first bad prediction samples for predicting high salary, we can see that the ngram model is confused by those beautiful words like "positive", "high profile" and "leadership", but ignores that it is describing civil-engineering industry which is a comparably low-salary industry. Comparing it to the first great prediction sample, both two descriptions have common in large amount of positive words, but the key difference is that the real high-salary position is describing a top-tier position in the world-leading luxury brand Chanel. Thus, increasing the weight of words in the industry can be a future improvement to the model.

In addition, vaguely defined job titles can be a key factor in misleading the judgement of the model. For example, the model is able to accurately judge obvious low-paying job titles such as "assistant" and "graduate", but will not distinguish between vaguely defined jobs such as "Sales" and "Negotiator". This is the mistake made in the second example of bad prediction samples for predicting low salaries. Sales in department stores and on the street will certainly pay low salaries, but in this example, how can flying to Dubai for real estate sales of top landmarks such as the Burj Khalifa and the Palm be paid low salaries? Thus, reinforcement learning for categorising vaguely bounded job vocabularies would also contribute significantly to improving the future accuracy of the model.




